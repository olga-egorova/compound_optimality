\documentclass[11pt]{article}

%%%%%%%%%%%%      PACKAGES        %%%%%%%%%%%%
\usepackage[colorlinks=true,linkcolor=blue,urlcolor=blue,citecolor=blue]{hyperref}
\usepackage{setspace}
\onehalfspacing
%\doublespacing
\usepackage{fancyhdr,afterpage}
\usepackage{lscape}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{textcomp,latexsym}
\usepackage{parskip}
\usepackage[round]{natbib}
\usepackage{adjustbox,lipsum}
\usepackage[titletoc]{appendix}
    
\usepackage[round]{natbib}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{gensymb}
\usepackage{tikz}
\usepackage{float}
%\usepackage{slashbox}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsfonts}
\usepackage{enumerate}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{bm}
\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage[a4paper,inner=4.2cm,outer=2.3cm,top=2.5cm,bottom=2.5cm,pdftex]{geometry} % MARGINS


\begin{document}
\tableofcontents

\newpage
\section*{Abstract}

\section{Introduction}

\textit{
	Model-independent approach to building designs in case of model uncertainty
	\begin{itemize}
		\item[-] Planning a controlled intervention -- reliance on a model (1) +
		\item[-] Active assumption of a specific model misspecification (2) =
		\item[-] Combining conflicting aims: model-based inference based on the model-independent variance estimate and distrust in the model sufficiency
		\item[-] Briefly: extension on more complex experimental structures
	\end{itemize}
}

Designed experiments are commonly conducted in order to gain understanding of the impact that different parameters of interest make on the outcome; the quantitative measure of this impact allows comparisons as well as making interpretable conclusions regarding the scale and the pattern of the relationships between process parameters and measured output. 

While the exact true nature of that relationship remains unknown, some form of approximation is needed -- and polynomial functions are able to provide any required precision for functions from a certain class of differentiability \citep{Rudin1987real}. Response Surface Methodology (RSM, \cite{Box1951Roy}) aims at optimising the approximated functions by fitting second-order polynomials, and higher desirable precision would require a polynomial of a higher order and, therefore, more experimental effort. 

Whichever the chosen, `primary',  model is, planning a controlled intervention does rely on the approximating model assumptions in two main -- and quite contradicting -- directions(?). Firstly, it is desirable to ensure the goodness of what the model is essentially fitted for, that is, for example, the precision of model parameters and/or the prediction accuracy of the untested treatment combinations. The corresponding design aims are usually reflected in utilising well-known optimality criteria (D-, A-, ?prediction) and rely on the variance estimate, the way it is obtained. Secondly, treating the chosen model as the absolute truth, especially at the designing stage is at least too optimistic and, as in many cases fitting a more suitable, but complicated model might not be feasible due to various restrictions and limitations, could be dangerous for the results credibility. So having a particular model also means that at the stage of planning it is highly desirable to include some [sensitivity towards and robustness against]  control over the model lack of fit and its effect on the inference. 

We deal with such duality [of model-dependence and accounting for its misspecification] by developing compound optimality criteria, each constructed as a weighted combination of individual criterion functions, with the two main features:
\begin{enumerate}
	\item Each corresponds to a specific desirable property:  either accounting for an aim coming from trusting the model or mitigating the effects from its potential misfit. The relative importance of the components are reflected by the assigned weights -- and we shall examine the performance of the resulting designs in terms of the individual criteria, and explore the role of weight allocation.
	\item The concept of model-independent internal variance estimation, `pure error' (\cite{GilmourTrinca2012})) underlies each of the individual criteria -- the most appropriate [sensible] strategy in the case of possible model insufficiency. 
 \end{enumerate}

The general spirit of this work aims at aligning with the concept of a good design, which, as summarised by \cite{Box1987empirical},  should ``make it possible to detect lack of fit'' and ``provide an internal estimate of error from replication'', among other properties. 

We will focus on factorial experiments with a relatively small number of runs and the fitted model being a polynomial regression. Section \ref{sec::background} provides the background on the modelling, error estimation and fundamental individual criteria. Controlling the lack-of-fit and the bias arising from the model misspecification are introduced in Section \ref{sec::criteria}, where they are combined with the primary model-driven ones in compound optimality criteria. Their adaptation to experimental frameworks with restricted randomisation is described in Section \ref{sec::ms_experiments}, as well as the design construction procedure for the multistratum experiments. Finally, a series of examples are presented in Section \ref{sec::examples} -- examining dynamics across various optimal designs,  shape of the constructed criteria and other properties, followed up by a Discussion in Section \ref{sec::discussion} with main conclusions and recommendations. 

The $\mbox{R}$ code used to search for the optimal designs are located at <...> [?github for now].

\section{Background}
\label{sec::background}

Recall the polynomial regression model (\ref{eq::back_model}) for the unblocked experiment, containing initial factors, their powers and interactions is used as a good approximation to the relation between the variables:  

\begin{equation}
\label{eq::back_model}
\bm{Y}=\bm{X\beta}+\bm{\varepsilon}.
\end{equation} 

Here $\bm{X}$ is the $n\times p$ model matrix, $\bm{Y}$ is the $n\times 1$ vector of responses; $\bm{\beta}$ is the $p\times 1$ vector of parameters corresponding to the model terms and $\bm{\varepsilon}$ are independent normally distributed random error terms with constant variance, i.e. $\bm{\varepsilon}\sim \mathcal{N}(\bm{0},\sigma^{2}\bm{I}_{n})$.

\subsection{Error estimation}
The error variance estimation can be obtained in two ways. The first one is the mean square error: $\hat{\sigma}^2_{mse}=\mbox{Residual SS}/(n-p)$, i.e. the residual sum of squares divided by the corresponding number of degrees of freedom (obtained from the ANOVA decomposition, the details can be found in \cite{Draper1998}). This estimator obviously depends on the model and on the number of its parameters. The other one is model-independent `pure' error, derived from the further decomposition of the residual sum of squares into the `pure' error and `lack-of-fit' components: $\hat{\sigma}^2_{PE}=\mbox{Pure error SS}/(n-t)$, where $t$ is the number of unique treatments applied and $d=n-t$ is the pure error degrees of freedom, i.e. the number of replicated points. In other words, the error is estimated by fitting the full treatment model:

\begin{equation}
\label{eq::back_trnt}
\bm{Y}=\bm{X_{t}\mu_{t}}+\bm{\varepsilon},
\end{equation} 

where $\bm{X_{t}}$ is the $n\times t$ full treatment model matrix, in which the $(i,j)^{th}$ element is equal to $1$ if treatment $j$ is applied to the $i^{th}$ unit, otherwise it is set to $0$. Then the elements of the $t$-dimensional vector $\bm{\mu_{t}}$ are the mean effects of each treatment. The vector of errors $\bm{\varepsilon}$ comprises the between-unit variation, such that $\mbox{E}(\bm{\varepsilon})=\bm{0}$, $\mbox{Var}(\bm{\varepsilon})=\sigma^2\bm{I}_{n}$.

Many authors advocate the use of the `pure' error estimate of $\sigma^2$ instead of the one pooled with the lack-of-fit part from the model (\ref{eq::back_model}): \cite{Cox1958planning} recommends using it for the estimation unless there are no replicate treatments, while \cite{Draper1998} argue for the reliability of `pure' error and recommend aiming for the presence of replicates at the stage of planning. \cite{Atkinson2007} also state that ``If lack-of-fit of the model is of potential interest, $\sigma^{2}$ is better estimated from replicate observations'' (page 22). For response surface experiments in blocks \cite{Gilmour2000PErsm} explicated the definition of pure error and its estimate which are compatible with the unblocked case: from replicates but also taking into account  block effects additive to treatment effects. Finally, the work by \cite{GilmourTrinca2012} that inspired this research, comprises a thorough analysis in favour of estimating the error from the full treatment model which is true regardless of what function is used to approximate the relationship of interest.



\subsection{Combining criteria}

The concept of the compound criteria is based on the notion of design efficiency, which can be defined for any design matrix and any criterion. For example, the $D$-efficiency of design $\bm{X}$ is
\begin{equation}
\label{eq::D_eff}
\mbox{Eff}_{D}(X)=\left[\frac{\vert \bm{X}'\bm{X}\vert}{\vert \bm{X}'_{*}\bm{X}_{*}\vert}\right]^{1/p},
\end{equation}   
where $\bm{X}_{*}$ is the $D$-optimum design. In this definition the power $1/p$ brings the efficiency to the scale of variances of model coefficients $\bm{\beta}_{i}, i=1\ldots p.$ It is obvious that the efficiency value may vary from $0$ to $1$ and is equal to $1$ if and only if the design is optimal according to the criterion of interest.

The final criterion function to be maximised among all the possible designs is obtained by combining the efficiencies for the component criteria $\mbox{Eff}_{1},\ldots, \mbox{Eff}_{m}$ with the corresponding weights $\kappa_{1},\ldots ,\kappa_{m}$ such that each $\kappa_{k}>0$ and $\sum_{k=1}^{m}\kappa_{k}=1:$

\begin{equation}
\label{eq::compound}
\mbox{Eff}^{\kappa_{1}}_{1}(\bm{X})\times\mbox{Eff}^{\kappa_{2}}_{2}(\bm{X})\times\ldots\times\mbox{Eff}^{\kappa_{m}}_{m}(\bm{X})\rightarrow \underset{\bm{X}}\max.
\end{equation}

The choice of weights is an arbitrary decision, made relying on both prior knowledge of the experimenter, the objectives of a specific experiment and the interpretation of the criterion components. The most general and intuitively sensible recommendation would be to obtain optimal designs with respect to several weight allocations, and then, after examining them, choose the one to be applied. Throughout the course of this research, when considering the examples of experiments layouts, we chose a set of weight allocations. Most of these sets were essentially classical design schemes for experiments with mixtures \citep{Cornell2011Mixtures}. For example, if the criterion consists of three elements, we would first obtain designs with all the weight on each component, then all combinations of distributing it equally between two components, and finally put equal weights on all of the components. Some additional schemes are included when examining particular cases.  

Some of the alternative approaches of considering several objectives would include the Pareto frontier approach, thoroughly described, for example, by \cite{Lu2011optimization}. When there are $l$ individual criterion functions $\phi_1(\bm{X}),\ldots\phi_{l}(\bm{X})$ to be maximised, the Pareto optimal set is generated so that for each of the designs from this set there is no other possible design that provides better criterion values for all the criteria (or, as it was defined, the Pareto optimal set is formed of designs that are not ``dominated'' by any other design).

Another approach was presented by \cite{Stallings2015general}: the authors developed methodology for generalising eigenvalue-based criteria (e.g. $A$- and $E$-optimality) in a way that allows differing interest (expressed through the weights) among any set of estimable functions of the fitted model parameters. The introduced strategy reflects the aims of experimentation that are not traditionally accounted for but definitely are of interest.

\section{Individual criteria construction}
\label{sec::criteria}
\input{compound_criterion}

\section{Restricted randomisation}
\label{sec::ms_experiments}
\input{ms_experiments}

\section{Examples}
\label{sec::examples}
\input{compound_examples}

%% Criteria for blocked experiments as a special case of MS
\subsection{Criteria for blocked experiments}
\label{subsec::compound_blocked}
\input{compound_blocked}

\subsection{Example. Case study}
\label{subsec::case_study}
\input{blocked_case_study}


\section{Discussion}
\label{sec::discussion}

\section{Acknowledgements}

%%% Bibliography
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Bibliography} 
%\bibliographystyle{apalike}
\bibliographystyle{rss}
\bibliography{thesis_bib}

\end{document}